{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Testing the FashionClip package for image characterization\n",
    "https://github.com/patrickjohncyh/fashion-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_clip.fashion_clip import FashionCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "from pathlib import Path\n",
    "image_paths = Path('../data/farfetch/images').glob('*.jpg')\n",
    "image_list = [str(p) for p in image_paths]\n",
    "print(f\"Found {len(image_list)} images, \", image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or when initializing FashionCLIP\n",
    "fclip = FashionCLIP('fashion-clip')\n",
    "# Ensure model is in float32\n",
    "fclip.model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create image embeddings and text embeddings\n",
    "image_embeddings = fclip.encode_images(image_list, batch_size=32)\n",
    "# text_embeddings = fclip.encode_text(texts, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Test weaviate search\n",
    "\n",
    "Taking one image of the H&M catalogue and looking for similar items in Farfetch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
    "WEAVIATE_KEY = os.getenv(\"WEAVIATE_KEY\")\n",
    "\n",
    "print(WEAVIATE_URL)\n",
    "print(WEAVIATE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_KEY),\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [{\n",
    "    \"image_name\": img,\n",
    "    \"fashion_clip_vector\": emb.tolist(),\n",
    "} for img, emb in zip(image_list, image_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes.config as wc\n",
    "\n",
    "if client.collections.exists(\"Farfetch2\"):\n",
    "    client.collections.delete(\"Farfetch2\")\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Farfetch2\",\n",
    "    properties=[\n",
    "        wc.Property(name=\"image_name\", data_type=wc.DataType.TEXT),\n",
    "    ],\n",
    "    vector_config=[\n",
    "        wc.Configure.Vectors.self_provided(\n",
    "            name=\"fashion_clip_vector\",\n",
    "            vector_index_config=wc.Configure.VectorIndex.hnsw(\n",
    "                distance_metric=wc.VectorDistances.COSINE\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "farfetch_collection = client.collections.get(\"Farfetch2\")\n",
    "\n",
    "with farfetch_collection.batch.dynamic() as batch:\n",
    "    for img_data in images:\n",
    "        batch.add_object(\n",
    "            properties={\"image_name\": img_data[\"image_name\"]}, \n",
    "            vector=img_data[\"fashion_clip_vector\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 =  \"../data/h-and-m-personalized-fashion-recommendations/images/093/0930409001.jpg\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(3, 3))\n",
    "\n",
    "img = mpimg.imread(image1)\n",
    "axes.imshow(img)\n",
    "axes.axis('off')  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = fclip.encode_images([image1], batch_size=1)\n",
    "query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = farfetch_collection.query.near_vector(\n",
    "    near_vector=query_embedding[0],\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for obj in response.objects:\n",
    "    print(obj.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    img = response.objects[i].properties[\"image_name\"]\n",
    "    img = mpimg.imread(img)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-mirror-on-cloud-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
