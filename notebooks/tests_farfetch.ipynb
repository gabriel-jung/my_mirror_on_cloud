{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Having a look at the farfetch catalog\n",
    "\n",
    "In particular, image taggings/descriptions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ollama\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import io\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Farfetch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "image_paths = Path('../data/farfetch/images').glob('*.jpg')\n",
    "image_list = list(image_paths)\n",
    "print(f\"Found {len(image_list)} images, \", image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_farfetch = pd.read_json(\"../data/farfetch/farfetch_fashion_dataset_images_crawlfeeds.json\")\n",
    "df_farfetch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Display first 25 images in 5x5 grid\n",
    "fig, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
    "axes = axes.flatten()  # Convert 2D array to 1D for easier indexing\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(image_list) and i < 25:  # Ensure we don't exceed available images\n",
    "        img = mpimg.imread(str(image_list[i]))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{image_list[i].name}\", fontsize=8)  # Optional: show filename\n",
    "        ax.axis('off')  # Remove axes\n",
    "    else:\n",
    "        ax.axis('off')  # Hide empty subplots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Testing Ollama for image description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = ollama.Client()\n",
    "\n",
    "# Check available models\n",
    "print(\"Available models:\", [m['model'] for m in client.list()['models']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Encode image to base64 for Ollama\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "def resize_and_encode_image(image_path, max_width=256):\n",
    "    \"\"\"Resize, then encode image to base64 string\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize if needed\n",
    "    if image.width > max_width:\n",
    "        ratio = max_width / image.width\n",
    "        new_height = int(image.height * ratio)\n",
    "        image = image.resize((max_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Display the resized image in notebook\n",
    "    # display(image)\n",
    "\n",
    "    # Convert to base64\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def test_model_performance(\n",
    "    model_name: str, encoded_image: str, system_prompt: str, user_prompt: str\n",
    ") -> Dict:\n",
    "    \"\"\"Test a single model and return results with timing\"\"\"\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt, \"images\": [encoded_image]},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"success\": True,\n",
    "            \"duration\": duration,\n",
    "            \"response\": response.message.content,\n",
    "            \"response_length\": len(response.message.content),\n",
    "            \"error\": None,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"success\": False,\n",
    "            \"duration\": None,\n",
    "            \"response\": None,\n",
    "            \"response_length\": 0,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "def run_model_comparison(image_path: str, models: List[str]) -> List[Dict]:\n",
    "    \"\"\"Run comparison across multiple models\"\"\"\n",
    "\n",
    "    # Define prompts\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant specialized in clothing and fashion analysis. \n",
    "    Follow instructions precisely, be concise and objective. \n",
    "    Only describe what is clearly visible in the image. \n",
    "    Provide structured responses exactly as requested.\"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"Analyze the clothing and fashion items in this image and provide:\n",
    "\n",
    "    1. ONE-WORD TAGS for each category (use commas to separate multiple tags):\n",
    "    - Colors: (e.g., red, blue, white)\n",
    "    - Clothing types: (e.g., shirt, pants, jacket, dress)\n",
    "    - Styles: (e.g., casual, formal, vintage, sporty)\n",
    "    - Occasions: (e.g., party, work, outdoor, evening)\n",
    "    - Materials: (e.g., cotton, leather, silk, denim)\n",
    "    - Patterns: (e.g., striped, floral, solid, plaid)\n",
    "    - Fits: (e.g., slim, loose, regular)\n",
    "    - Seasons: (e.g., summer, winter, all-season)\n",
    "\n",
    "    2. DESCRIPTIONS: One concise sentence per distinct clothing item, highlighting color, style, and key features.\n",
    "\n",
    "    Only include tags you can clearly observe. Be precise and objective. Format your response clearly.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(f\"Testing {len(models)} models on image: {image_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    encoded_image = resize_and_encode_image(image_path)\n",
    "\n",
    "    for model_name in models:\n",
    "        # print(f\"\\nðŸ” Testing: {model_name}\")\n",
    "\n",
    "        result = test_model_performance(\n",
    "            model_name, encoded_image, system_prompt, user_prompt\n",
    "        )\n",
    "        result[\"image\"] = image_path.name\n",
    "        results.append(result)\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            print(f\"â±ï¸  Time: {result['duration']:.2f}s\")\n",
    "            print(f\"ðŸ“ Response length: {result['response_length']} characters\")\n",
    "            print(f\"ðŸ’¬ Response preview: {result['response'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"âŒ Error: {result['error']}\")\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Usage\n",
    "def main(image_path):\n",
    "    models = [\n",
    "        \"gemma3:4b\",\n",
    "        \"granite3.2-vision:2b\",  \n",
    "        \"qwen2.5vl:3b\",\n",
    "        \"llava:7b\",\n",
    "        \"qwen2.5vl:7b\",\n",
    "        \"llama3.2-vision:latest\",\n",
    "    ]\n",
    "\n",
    "    # Run tests\n",
    "    results = run_model_comparison(image_path, models)\n",
    "\n",
    "    # Summary analysis\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“Š PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    successful_results = [r for r in results if r[\"success\"]]\n",
    "\n",
    "    if successful_results:\n",
    "        # Sort by speed\n",
    "        successful_results.sort(key=lambda x: x[\"duration\"])\n",
    "\n",
    "        print(\"\\nðŸƒ Speed Ranking (fastest first):\")\n",
    "        for i, result in enumerate(successful_results, 1):\n",
    "            print(f\"{i}. {result['model']}: {result['duration']:.2f}s\")\n",
    "\n",
    "        # Response quality comparison\n",
    "        print(\"\\nðŸ“ Response Length Comparison:\")\n",
    "        for result in successful_results:\n",
    "            print(f\"{result['model']}: {result['response_length']} chars\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = main(image_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results.to_csv(\"../results/model_comparison_results.csv\", index=False)\n",
    "all_results = pd.read_csv(\"../results/model_comparison_results.csv\")\n",
    "all_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    all_results,\n",
    "    x=all_results.index,\n",
    "    y=\"duration\",\n",
    "    color=\"model\",\n",
    "    title=\"Model Performance Over Test Cases\",\n",
    "    labels={\n",
    "        \"duration\": \"Response Time (seconds)\",\n",
    "        \"index\": \"Test Case Index\",\n",
    "        \"model\": \"Model\"\n",
    "    },\n",
    "    hover_data=[\"image\", \"success\", \"response_length\"],\n",
    "    range_y=[0,60]\n",
    ")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"Colors\", \"Clothing types\", \"Styles\", \"Occasions\", \"Materials\", \"Patterns\", \"Fits\", \"Seasons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tag_values(response, tag_keys):\n",
    "#     \"\"\"Safely extract tag values, handling None or empty responses\"\"\"\n",
    "#     extracted = {}\n",
    "    \n",
    "#     if not response:\n",
    "#         print(0)\n",
    "#         return {tag: [] for tag in tag_keys}\n",
    "        \n",
    "#     for tag in tag_keys:\n",
    "#         if tag + \":\" in response:\n",
    "#             tag_response = response.split(tag + \":\")\n",
    "#             if len(tag_response) > 1:\n",
    "#                 value = tag_response[1].split(\";\")[0].split(\"\\n\", 1)[0].strip()\n",
    "#                 values = [v.strip().lower() for v in value.split(\",\")]\n",
    "#                 extracted[tag] = values\n",
    "#             else:\n",
    "#                 extracted[tag] = []\n",
    "#         else:\n",
    "#             extracted[tag] = []\n",
    "#     return extracted\n",
    "\n",
    "def extract_tag_values(response, tag_keys):\n",
    "    \"\"\"Safely extract tag values, handling None, NaN, or empty responses\"\"\"\n",
    "    extracted = {tag: [] for tag in tag_keys}  # Initialize with empty lists\n",
    "    \n",
    "    # Handle None, NaN, or non-string values\n",
    "    if not response or not isinstance(response, str):\n",
    "        return extracted\n",
    "        \n",
    "    for tag in tag_keys:\n",
    "        if tag + \":\" in response:\n",
    "            tag_response = response.split(tag + \":\")\n",
    "            if len(tag_response) > 1:\n",
    "                value = tag_response[1].split(\";\")[0].split(\"\\n\", 1)[0].strip()\n",
    "                values = [v.strip().lower() for v in value.split(\",\")]\n",
    "                extracted[tag] = values\n",
    "            else:\n",
    "                extracted[tag] = []\n",
    "        else:\n",
    "            extracted[tag] = []\n",
    "    return extracted\n",
    "\n",
    "\n",
    "# Use the safe function\n",
    "all_tags = all_results['response'].apply(lambda x: extract_tag_values(x, tags))\n",
    "all_tags_df = pd.DataFrame(all_tags.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results_with_tags = pd.concat([all_results.reset_index(drop=True), all_tags_df], axis=1)\n",
    "all_results_with_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import os\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "def pil_image_to_base64_str(img):\n",
    "    \"\"\"Convert PIL Image to base64 string for HTML embedding\"\"\"\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    return img_str\n",
    "\n",
    "def create_simple_comparison_table(df, image_name, tag_columns):\n",
    "    \"\"\"Create comparison table with tags as rows and models as columns\"\"\"\n",
    "    image_data = df[df['image'] == image_name]\n",
    "    comparison_data = {}\n",
    "    \n",
    "    for _, row in image_data.iterrows():\n",
    "        model = row['model']\n",
    "        model_tags = {}\n",
    "        for tag in tag_columns:\n",
    "            if tag in row:\n",
    "                model_tags[tag] = row[tag] \n",
    "            else:\n",
    "                model_tags[tag] = \"No data\"\n",
    "        comparison_data[model] = model_tags\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df\n",
    "\n",
    "def display_image_and_table_side_by_side(image_path, comparison_df, width=300):\n",
    "    \"\"\"Display image and comparison table side by side using HTML\"\"\"\n",
    "    \n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        display(comparison_df)\n",
    "        return\n",
    "\n",
    "    # Load and resize image\n",
    "    img = PILImage.open(image_path)\n",
    "    img.thumbnail((width, width), PILImage.LANCZOS)\n",
    "    img_data = pil_image_to_base64_str(img)\n",
    "\n",
    "    # Create side-by-side HTML layout\n",
    "    html = f'''\n",
    "    <div style=\"display: flex; align-items: flex-start; gap: 20px; margin-bottom: 40px; \n",
    "                border: 1px solid #ddd; padding: 15px; border-radius: 8px;\">\n",
    "      <div style=\"flex-shrink: 0;\">\n",
    "        <h4 style=\"margin-top: 0; color: #333;\">{os.path.basename(image_path)}</h4>\n",
    "        <img src=\"data:image/png;base64,{img_data}\" alt=\"Clothing Image\" \n",
    "             style=\"max-width: {width}px; height: auto; border-radius: 8px; \n",
    "                    border: 2px solid #eee; box-shadow: 0 2px 8px rgba(0,0,0,0.1);\"/>\n",
    "      </div>\n",
    "      <div style=\"flex-grow: 1; overflow-x: auto;\">\n",
    "        <h4 style=\"margin-top: 0; color: #333;\">Model Comparison</h4>\n",
    "        {comparison_df.to_html(border=0, table_id='comparison_table', \n",
    "                               classes='table table-striped', escape=False)}\n",
    "      </div>\n",
    "    </div>\n",
    "    '''\n",
    "    display(HTML(html))\n",
    "\n",
    "# Your improved main loop\n",
    "tags = [\"Colors\", \"Clothing types\", \"Styles\", \"Occasions\", \"Materials\", \"Patterns\", \"Fits\", \"Seasons\"]\n",
    "unique_images = all_results_with_tags['image'].unique()\n",
    "\n",
    "for img in unique_images[:50]:\n",
    "    image_path = f\"../data/farfetch/images/{img}\"\n",
    "    simple_table = create_simple_comparison_table(all_results_with_tags, img, tags)\n",
    "    display_image_and_table_side_by_side(image_path, simple_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_colors = [color for sublist in all_results_with_tags['Colors'] for color in sublist if sublist]\n",
    "color_counts = pd.Series(all_colors).value_counts().reset_index()\n",
    "color_counts.columns = ['color', 'count']\n",
    "color_counts\n",
    "\n",
    "# Create bar plot with plotly express\n",
    "fig = px.bar(\n",
    "    color_counts[color_counts[\"count\"] > 5], \n",
    "    x='color', \n",
    "    y='count',\n",
    "    title='Frequency of Clothing Colors Across All Responses',\n",
    "    labels={'color': 'Color', 'count': 'Frequency'},\n",
    "    color='color'  # Optional: color bars by the color names\n",
    ")\n",
    "\n",
    "# fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels if needed\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types = [t for sublist in all_results_with_tags['Clothing types'] for t in sublist if sublist]\n",
    "type_counts = pd.Series(all_types).value_counts().reset_index()\n",
    "type_counts.columns = ['type', 'count']\n",
    "type_counts\n",
    "\n",
    "# Create bar plot with plotly express\n",
    "fig = px.bar(\n",
    "    type_counts[type_counts[\"count\"] > 5], \n",
    "    x='type', \n",
    "    y='count',\n",
    "    title='Frequency of Clothing Types Across All Responses',\n",
    "    labels={'type': 'Type', 'count': 'Frequency'},\n",
    "    color='type'  # Optional: color bars by the type names\n",
    ")\n",
    "\n",
    "# fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels if needed\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seasons = [t for sublist in all_results_with_tags['Seasons'] for t in sublist if sublist]\n",
    "season_counts = pd.Series(all_seasons).value_counts().reset_index()\n",
    "season_counts.columns = ['season', 'count']\n",
    "season_counts\n",
    "\n",
    "# Create bar plot with plotly express\n",
    "fig = px.bar(\n",
    "    season_counts[season_counts[\"count\"] > 5], \n",
    "    x='season', \n",
    "    y='count',\n",
    "    title='Frequency of Clothing Seasons Across All Responses',\n",
    "    labels={'season': 'Season', 'count': 'Frequency'},\n",
    "    color='season'  # Optional: color bars by the season names\n",
    ")\n",
    "\n",
    "# fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels if needed\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_images = all_results_with_tags['image'].unique()\n",
    "for img in unique_images:\n",
    "    print(f\"Image: {img}\")\n",
    "    subset = all_results_with_tags[all_results_with_tags['image'] == img]\n",
    "    for _, row in subset.iterrows():\n",
    "        print(f\"  {row['model']} -> {row['Colors']}\")\n",
    "    print(\"-\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(desc_text):\n",
    "    \"\"\"Clean and normalize description text to consistent format\"\"\"\n",
    "    # Remove empty lines and split by newlines\n",
    "    lines = [line.strip() for line in desc_text.splitlines() if line.strip()]\n",
    "    \n",
    "    # Remove bullet points, asterisks, dashes, and numbered lists\n",
    "    lines = [re.sub(r'^[-*â€¢\\d\\.]+\\s*', '', line) for line in lines]\n",
    "    \n",
    "    # Remove any remaining formatting markers like ** or ***\n",
    "    lines = [re.sub(r'\\*+', '', line).strip() for line in lines]\n",
    "    \n",
    "    # Join all sentences into a single paragraph\n",
    "    cleaned_text = ' '.join(lines)\n",
    "    \n",
    "    # Add period at the end if missing\n",
    "    if cleaned_text and cleaned_text[-1] not in '.!?':\n",
    "        cleaned_text += '.'\n",
    "        \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,50,100,150]:\n",
    "    parts = re.split(\n",
    "        r\"descriptions:\", all_results.iloc[i][\"response\"], flags=re.IGNORECASE, maxsplit=1\n",
    "    )\n",
    "    desc = [part.strip() for part in parts][-1]\n",
    "    print(len(desc))\n",
    "    if len(desc)>1:\n",
    "        print(desc)\n",
    "        print(clean_descriptions(desc))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## More structured tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Run analysis on full catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = [\n",
    "    \"Black\",\n",
    "    \"White\",\n",
    "    \"Off White\",\n",
    "    \"Light Beige\",\n",
    "    \"Beige\",\n",
    "    \"Grey\",\n",
    "    \"Light Blue\",\n",
    "    \"Light Grey\",\n",
    "    \"Dark Blue\",\n",
    "    \"Dark Grey\",\n",
    "    \"Pink\",\n",
    "    \"Dark Red\",\n",
    "    \"Greyish Beige\",\n",
    "    \"Light Orange\",\n",
    "    \"Silver\",\n",
    "    \"Gold\",\n",
    "    \"Light Pink\",\n",
    "    \"Dark Pink\",\n",
    "    \"Yellowish Brown\",\n",
    "    \"Blue\",\n",
    "    \"Light Turquoise\",\n",
    "    \"Yellow\",\n",
    "    \"Greenish Khaki\",\n",
    "    \"Dark Yellow\",\n",
    "    \"Other Pink\",\n",
    "    \"Dark Purple\",\n",
    "    \"Red\",\n",
    "    \"Transparent\",\n",
    "    \"Dark Green\",\n",
    "    \"Other Red\",\n",
    "    \"Turquoise\",\n",
    "    \"Dark Orange\",\n",
    "    \"Orange\",\n",
    "    \"Dark Beige\",\n",
    "    \"Other Yellow\",\n",
    "    \"Light Green\",\n",
    "    \"Other Orange\",\n",
    "    \"Purple\",\n",
    "    \"Light Red\",\n",
    "    \"Light Yellow\",\n",
    "    \"Green\",\n",
    "    \"Light Purple\",\n",
    "    \"Dark Turquoise\",\n",
    "    \"Other Purple\",\n",
    "    \"Bronze/Copper\",\n",
    "    \"Other Turquoise\",\n",
    "    \"Other Green\",\n",
    "    \"Other Blue\",\n",
    "    # \"Unknown\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\"Men\", \"Women\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [\n",
    "    \"Trousers\",\n",
    "    \"Dress\",\n",
    "    \"Sweater\",\n",
    "    \"T-shirt\",\n",
    "    \"Top\",\n",
    "    \"Blouse\",\n",
    "    \"Jacket\",\n",
    "    \"Shorts\",\n",
    "    \"Shirt\",\n",
    "    \"Vest top\",\n",
    "    \"Underwear bottom\",\n",
    "    \"Skirt\",\n",
    "    \"Hoodie\",\n",
    "    \"Bra\",\n",
    "    \"Socks\",\n",
    "    \"Leggings/Tights\",\n",
    "    \"Shoes\",\n",
    "    \"Cardigan\",\n",
    "    \"Hat/beanie\",\n",
    "    \"Pyjama set\",\n",
    "    \"Blazer\",\n",
    "    \"Scarf\",\n",
    "    \"Swimsuit\",\n",
    "    \"Coat\",\n",
    "    \"Belt\",\n",
    "    \"Polo shirt\",\n",
    "    \"Gloves\",\n",
    "    \"Tie\",\n",
    "    \"Robe\",\n",
    "    # \"Other\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flat_list = list(itertools.chain.from_iterable(all_results_with_tags[\"Styles\"].to_list()))\n",
    "# set(flat_list)\n",
    "styles = {\n",
    "    'artsy', 'athletic', 'beachwear', 'biker', 'bohemian', 'business',\n",
    "    'casual', 'chic', 'classic', 'cocktail', 'edgy', 'elegant', \n",
    "    'evening', 'fitted', 'flowy', 'formal', 'gothic', 'graphic', \n",
    "    'grunge', 'knit', 'maxi', 'minimalist', 'modern', 'monogrammed', \n",
    "    'oversized', 'preppy', 'punk', 'romantic', 'retro', 'sneaker', \n",
    "    'sporty', 'streetwear', 'tropical', 'urban', 'vintage', 'workwear', \n",
    "    # 'unknown'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = list(itertools.chain.from_iterable(all_results_with_tags[\"Occasions\"].to_list()))\n",
    "# set(flat_list)\n",
    "\n",
    "occasions = {\n",
    "    'athletic', 'brunch', 'casual', 'cocktail', 'date', 'daytime', \n",
    "    'dinner', 'evening', 'everyday', 'formal', 'funeral', \n",
    "    'graduation', 'holiday', 'interview', 'loungewear', 'maternity', \n",
    "    'networking', 'outdoor', 'party', 'religious', 'sleepwear', \n",
    "    'travel', 'vacation', 'wedding', 'work', \n",
    "    # 'unknown'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = list(itertools.chain.from_iterable(all_results_with_tags[\"Fits\"].to_list()))\n",
    "# set(flat_list)\n",
    "\n",
    "fits = {\n",
    "    'athletic', 'boxy', 'compact', 'cropped', 'fitted', 'flowing', \n",
    "    'form-fitting', 'oversized', 'regular', 'relaxed', 'skinny', \n",
    "    'slim', 'straight', 'tailored', 'wide', \n",
    "    # 'unknown'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = {'spring', 'summer', 'fall', 'winter'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = list(itertools.chain.from_iterable(all_results_with_tags[\"Materials\"].to_list()))\n",
    "# set(flat_list)\n",
    "\n",
    "materials = {\n",
    "    'bamboo', 'canvas', 'cashmere', 'chiffon', 'corduroy', 'cotton', \n",
    "    'denim', 'fleece', 'hemp', 'jersey', 'knit', 'leather', 'linen', \n",
    "    'lycra', 'mesh', 'metal', 'modal', 'neoprene', 'nylon', 'plaid', \n",
    "    'plastic', 'polyester', 'rayon', 'rubber', 'satin', 'silk', \n",
    "    'spandex', 'suede', 'synthetic', 'tencel', 'twill', 'velvet', \n",
    "    'viscose', 'wool', 'unknown'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = list(itertools.chain.from_iterable(all_results_with_tags[\"Patterns\"].to_list()))\n",
    "# set(flat_list)\n",
    "\n",
    "patterns = {\n",
    "    'abstract', 'argyle', 'checkered', 'floral', 'graphic', \n",
    "    'logo', 'monogram', 'paisley', 'plaid', 'solid', 'striped'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cleaned categories from our analysis\n",
    "CLOTHING_CATEGORIES = {\n",
    "    \"types\": types,\n",
    "    \"genres\": genres,\n",
    "    \"colours\": colours,\n",
    "    \"styles\": styles,\n",
    "    \"occasions\": occasions,\n",
    "    \"fits\": fits,\n",
    "    \"materials\": materials,\n",
    "    \"patterns\": patterns,\n",
    "    \"seasons\": seasons,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(clothing_categories):\n",
    "    \"\"\"Create system prompt with proper category enforcement and description field.\"\"\"\n",
    "    \n",
    "    prompt = \"\"\"You are a clothing analysis AI. Return ONLY valid JSON, no other text.\n",
    "\n",
    "STRICT RULES: You MUST only use tags from these exact lists and include a description field as a string.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add each category with its exact allowed values\n",
    "    for category, items in clothing_categories.items():\n",
    "        if isinstance(items, (set, list)):\n",
    "            items_str = ', '.join(f'\"{item}\"' for item in sorted(items))\n",
    "            prompt += f'{category.upper()}: [{items_str}]\\n\\n'\n",
    "\n",
    "    prompt += f\"\"\"CRITICAL:\n",
    "- Use ONLY the tags from the list: {list(clothing_categories.keys())}\n",
    "- Each category must be a list of strings\n",
    "- Add a \"description\" field with a short textual description of the clothing in the image\n",
    "- If unsure about a category, use empty list []\n",
    "- Return JSON only, no explanations\n",
    "\n",
    "STRICT: Do not use any keys other than the exact category names provided.\n",
    "Do not use combined or generic keys like \"categories\".\n",
    "Every category must be present, even if empty like [].\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_user_prompt():\n",
    "    \"\"\"User prompt asking for JSON with categories and a short description.\"\"\"\n",
    "    return \"\"\"Analyze this clothing image. Return JSON with the 9 categories as lists of strings and a \"description\" field with a concise summary of the clothing. Use only allowed tags. JSON only, no other text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_json_simple(response_text: str) -> dict:\n",
    "    \"\"\"Just parse JSON as-is, no cleaning\"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            'success': True,\n",
    "            'data': json.loads(response_text)\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'data': None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_encode_image(image_path, max_width=256):\n",
    "    \"\"\"Resize and encode image\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    if image.width > max_width:\n",
    "        ratio = max_width / image.width\n",
    "        new_height = int(image.height * ratio)\n",
    "        image = image.resize((max_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name: str, encoded_image: str, system_prompt: str, user_prompt: str) -> dict:\n",
    "    \"\"\"Test one model\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt, \"images\": [encoded_image]},\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        parsed = parse_json_simple(response.message.content)\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"duration\": duration,\n",
    "            \"response\": response.message.content,\n",
    "            \"data\": parsed['data'],\n",
    "            \"json_success\": parsed['success']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"error\": str(e),\n",
    "            \"data\": None,\n",
    "            \"json_success\": False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_block(response_text):\n",
    "    if \"categories\" in response_text.lower():\n",
    "        print(i)\n",
    "    match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1).replace('[\"\"]', '[]').replace('[\"]', '[]').lower()\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Failed to parse JSON: {e}\")\n",
    "    else:\n",
    "        raise ValueError(\"JSON block not found\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 10\n",
    "\n",
    "def save_results(result, filename):\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
    "        json_line = json.dumps(result, ensure_ascii=False)\n",
    "        f.write(json_line + \"\\n\")\n",
    "\n",
    "def is_valid_structure(data, expected_keys):\n",
    "    # Check keys exactly match expected, no extras or missing ones\n",
    "    data_keys = set(data.keys())\n",
    "    expected_keys = set(expected_keys)\n",
    "    return data_keys == expected_keys\n",
    "\n",
    "def call_ai_and_validate(model_name, encoded_image, system_prompt, user_prompt, expected_keys):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        response_text = test_model(model_name, encoded_image, system_prompt, user_prompt)  \n",
    "        json_data = extract_json_block(response_text[\"response\"])\n",
    "        \n",
    "        if json_data and is_valid_structure(json_data, expected_keys):\n",
    "            return response_text | json_data \n",
    "        else:\n",
    "            print(f\"Invalid response structure on attempt {attempt+1}, retrying...\")\n",
    "    raise ValueError(\"Failed to get valid JSON structure after retries\")\n",
    "\n",
    "def run_analysis_batch(image_paths: List[str], clothing_categories: dict, output_file: str) -> List[dict]:\n",
    "    \"\"\"Run analysis on multiple images with tqdm progress bar\"\"\"\n",
    "    \n",
    "    models = [\"llava:7b\", \"qwen2.5vl:7b\"]\n",
    "    expected_keys = list(CLOTHING_CATEGORIES.keys()) + [\"description\"]\n",
    "    \n",
    "    system_prompt = create_system_prompt(clothing_categories)\n",
    "    user_prompt = create_user_prompt()\n",
    "        \n",
    "    results = []\n",
    "    \n",
    "    for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        try:\n",
    "            encoded_image = resize_and_encode_image(image_path)\n",
    "            \n",
    "            for model_name in models:\n",
    "                try:\n",
    "                    result = call_ai_and_validate(model_name, encoded_image, system_prompt, user_prompt, expected_keys)\n",
    "                    result[\"image_name\"] = Path(image_path).name\n",
    "                    result[\"image_path\"] = str(image_path)\n",
    "                    result[\"json_success\"] = True\n",
    "                    results.append(result)\n",
    "                except ValueError as e:\n",
    "                    # Max retries reached, log and continue\n",
    "                    result = {\n",
    "                        \"model\": model_name,\n",
    "                        \"image_name\": Path(image_path).name,\n",
    "                        \"image_path\": str(image_path),\n",
    "                        \"error\": f\"Max retries reached: {str(e)}\",\n",
    "                        \"data\": None,\n",
    "                        \"json_success\": False\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                save_results(result, filename=output_file)\n",
    "           \n",
    "        except Exception as e:\n",
    "            \n",
    "            # Handle image processing errors\n",
    "            for model_name in models:\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"image_name\": Path(image_path).name,\n",
    "                    \"image_path\": str(image_path),\n",
    "                    \"error\": f\"Image processing error: {str(e)}\",\n",
    "                    \"data\": None,\n",
    "                    \"json_success\": False\n",
    "                })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_analysis_batch(image_list[:], CLOTHING_CATEGORIES, output_file=\"../results/farfetch.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Read results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../results/farfetch.jsonl\", lines=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_empty_list(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return []\n",
    "    # Handle string \"NaN\" (case-insensitive, with whitespace handling)\n",
    "    if isinstance(x, str) and x.strip().lower() == 'nan':\n",
    "        return []\n",
    "    return x\n",
    "\n",
    "# Apply to all columns\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(replace_nan_with_empty_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_stats(df):\n",
    "    perfect_matches = 0\n",
    "    total_images = 0\n",
    "    results = []  # Store detailed results\n",
    "    \n",
    "    for d in df[\"image_name\"].unique():\n",
    "        image_colours = df[df[\"image_name\"] == d][\"colours\"]\n",
    "        \n",
    "        if len(image_colours) < 2:\n",
    "            continue\n",
    "            \n",
    "        total_images += 1\n",
    "        \n",
    "        try:\n",
    "            set1 = set(image_colours.iloc[0])\n",
    "            set2 = set(image_colours.iloc[1])\n",
    "            \n",
    "            if set1 == set2:\n",
    "                perfect_matches += 1\n",
    "                results.append({\"image_name\": d, \"match_type\": \"perfect\", \"similarity\": 1.0})\n",
    "            else:\n",
    "                intersect = set1 & set2\n",
    "                union = set1 | set2\n",
    "                similarity = len(intersect) / len(union) if len(union) > 0 else 0\n",
    "                results.append({\"image_name\": d, \"match_type\": \"partial\", \"similarity\": similarity})\n",
    "                \n",
    "        except (TypeError, AttributeError):\n",
    "            results.append({\"image_name\": d, \"match_type\": \"error\", \"similarity\": 0})\n",
    "    \n",
    "    perfect_percentage = (perfect_matches / total_images * 100) if total_images > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"perfect_matches\": perfect_matches,\n",
    "        \"total_images\": total_images,\n",
    "        \"perfect_percentage\": perfect_percentage,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "stats = calculate_match_stats(df)\n",
    "print(f\"Perfect match percentage: {stats['perfect_percentage']:.2f}%\")\n",
    "print(f\"Perfect matches: {stats['perfect_matches']}\")\n",
    "print(f\"Total valid images: {stats['total_images']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_image_match_database(df):\n",
    "    records = []\n",
    "\n",
    "    for d in df[\"image_name\"].unique():\n",
    "        record = {\"image_name\": d}  # Start with image name\n",
    "        for tag in CLOTHING_CATEGORIES.keys():\n",
    "            tag_lower = tag.lower()\n",
    "            image_tags = df[df[\"image_name\"] == d][tag_lower]\n",
    "\n",
    "            # Handle cases with insufficient data\n",
    "            if len(image_tags) < 2:\n",
    "                record[f\"{tag_lower}_intersection\"] = []\n",
    "                record[f\"{tag_lower}_union\"] = []\n",
    "                record[f\"{tag_lower}_match_percentage\"] = 0.0\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                set1 = set(image_tags.iloc[0])\n",
    "                set2 = set(image_tags.iloc[1])\n",
    "\n",
    "                intersect = list(set1 & set2)\n",
    "                union = list(set1 | set2)\n",
    "                match_percentage = (len(intersect) / len(union)) * 100 if union else 0\n",
    "\n",
    "                # Add to the SAME record\n",
    "                record[f\"{tag_lower}_intersection\"] = intersect\n",
    "                record[f\"{tag_lower}_union\"] = union\n",
    "                record[f\"{tag_lower}_match_percentage\"] = match_percentage\n",
    "\n",
    "            except (TypeError, AttributeError):\n",
    "                record[f\"{tag_lower}_intersection\"] = []\n",
    "                record[f\"{tag_lower}_union\"] = []\n",
    "                record[f\"{tag_lower}_match_percentage\"] = 0.0\n",
    "\n",
    "        records.append(record)  # Append complete record ONCE per image\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Create the database\n",
    "image_match_db = create_image_match_database(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_match_db.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [col for col in image_match_db.columns if col.endswith('_match_percentage')]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "categories = [col for col in image_match_db.columns if col.endswith('_match_percentage')][:9]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=3,\n",
    "    subplot_titles=[cat.replace('_match_percentage', '').replace('_', ' ').title() for cat in categories],\n",
    "    specs=[[{\"secondary_y\": False}]*3 for _ in range(3)],\n",
    "    horizontal_spacing=0.08,\n",
    "    vertical_spacing=0.12\n",
    ")\n",
    "\n",
    "# Define a color palette with 9 distinct colors (Plotly default colors)\n",
    "colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A', \n",
    "          '#19D3F3', '#FF6692', '#B6E880', '#FF97FF']\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=image_match_db[cat],\n",
    "            nbinsx=15,\n",
    "            histnorm='percent',  # Normalize to 100%\n",
    "            name=cat.replace('_match_percentage', ''),\n",
    "            showlegend=False,\n",
    "            marker_color=colors[i],\n",
    "            opacity=0.8,\n",
    "            marker_line_width=1,\n",
    "            marker_line_color='white'\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Match Percentage Distributions (Normalized)\",\n",
    "    height=900,\n",
    "    width=1200,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_match_db.to_csv(\"../results/farfetch_image_match_database.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Further tests: unique colour extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\n",
    "for items in colours:\n",
    "        prompt += f' [{items}]\\n\\n'\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt():\n",
    "    \"\"\"Create system prompt with strict one-word enforcement.\"\"\"\n",
    "    prompt = f\"\"\"You are a clothing color detection AI. You MUST return EXACTLY one word.\n",
    "\n",
    "STRICT RULES:\n",
    "- Return ONLY one color from this list: {colours}\n",
    "- NO punctuation, NO explanations, NO sentences\n",
    "- NO multiple colors (e.g., NOT \"red and blue\")\n",
    "- NO descriptors (e.g., NOT \"dark blue\", just \"blue\")\n",
    "- NO phrases (e.g., NOT \"reddish brown\", just \"red\")\n",
    "- If uncertain between colors, pick the most dominant one\n",
    "- ONLY output the single color word\n",
    "\n",
    "Example responses: \"red\", \"blue\", \"green\" (NOT \"The main color is red\")\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def create_user_prompt():\n",
    "    \"\"\"User prompt with additional constraints.\"\"\"\n",
    "    return \"\"\"What is the main color? Answer with ONE WORD ONLY from the approved color list.\"\"\"\n",
    "\n",
    "\n",
    "def parse_json_simple(response_text: str) -> dict:\n",
    "    \"\"\"Just parse JSON as-is, no cleaning\"\"\"\n",
    "    try:\n",
    "        return {\"success\": True, \"data\": json.loads(response_text)}\n",
    "    except:\n",
    "        return {\"success\": False, \"data\": None}\n",
    "\n",
    "\n",
    "def resize_and_encode_image(image_path, max_width=256):\n",
    "    \"\"\"Resize and encode image\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    if image.width > max_width:\n",
    "        ratio = max_width / image.width\n",
    "        new_height = int(image.height * ratio)\n",
    "        image = image.resize((max_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def test_model(\n",
    "    model_name: str, encoded_image: str, system_prompt: str, user_prompt: str\n",
    ") -> dict:\n",
    "    \"\"\"Test one model\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt, \"images\": [encoded_image]},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        parsed = parse_json_simple(response.message.content)\n",
    "\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"duration\": duration,\n",
    "            \"response\": response.message.content,\n",
    "            \"data\": parsed[\"data\"],\n",
    "            \"json_success\": parsed[\"success\"],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"error\": str(e),\n",
    "            \"data\": None,\n",
    "            \"json_success\": False,\n",
    "        }\n",
    "\n",
    "\n",
    "def call_ai_and_validate(model_name, encoded_image, system_prompt, user_prompt):\n",
    "    response_text = test_model(model_name, encoded_image, system_prompt, user_prompt)\n",
    "    return response_text\n",
    "\n",
    "\n",
    "def run_analysis_batch(image_paths: List[str]) -> List[dict]:\n",
    "    \"\"\"Run analysis on multiple images with tqdm progress bar\"\"\"\n",
    "\n",
    "    models = [\"llava:7b\", \"qwen2.5vl:7b\"]\n",
    "\n",
    "    system_prompt = create_system_prompt()\n",
    "    user_prompt = create_user_prompt()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        try:\n",
    "            encoded_image = resize_and_encode_image(image_path)\n",
    "\n",
    "            for model_name in models:\n",
    "                try:\n",
    "                    result = call_ai_and_validate(\n",
    "                        model_name, encoded_image, system_prompt, user_prompt\n",
    "                    )\n",
    "                    result[\"image_name\"] = Path(image_path).name\n",
    "                    result[\"image_path\"] = str(image_path)\n",
    "                    result[\"json_success\"] = True\n",
    "                    results.append(result)\n",
    "                except ValueError as e:\n",
    "                    # Max retries reached, log and continue\n",
    "                    result = {\n",
    "                        \"model\": model_name,\n",
    "                        \"image_name\": Path(image_path).name,\n",
    "                        \"image_path\": str(image_path),\n",
    "                        \"error\": f\"Max retries reached: {str(e)}\",\n",
    "                        \"data\": None,\n",
    "                        \"json_success\": False,\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                # save_results(result, filename=output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle image processing errors\n",
    "            for model_name in models:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"image_name\": Path(image_path).name,\n",
    "                        \"image_path\": str(image_path),\n",
    "                        \"error\": f\"Image processing error: {str(e)}\",\n",
    "                        \"data\": None,\n",
    "                        \"json_success\": False,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_analysis_batch(image_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## kNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogue = pd.read_csv(\"../results/farfetch_image_match_database.csv\")\n",
    "df_catalogue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # Convert all string list columns to actual lists upfront\n",
    "def convert_string_lists_to_actual_lists(df, list_columns):\n",
    "    \"\"\"Convert string representations of lists to actual lists\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in list_columns:\n",
    "        df_clean[col] = df_clean[col].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else x\n",
    "        )\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean your data first\n",
    "cols = df_catalogue.columns[df_catalogue.columns.str.contains(\"union\") | df_catalogue.columns.str.contains(\"intersection\")].to_list()\n",
    "\n",
    "df_clean = convert_string_lists_to_actual_lists(df_catalogue, cols)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"genres\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"colours\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set([c.split('_')[0] for c in df_clean.columns])\n",
    "tags.remove('image')\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def encode_list_column(df, tag, suffix='_union'):\n",
    "    \"\"\"Properly encode a pandas series containing lists into individual binary columns\"\"\"\n",
    "    \n",
    "    # Use MultiLabelBinarizer - this is the correct tool for the job\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    encoded = mlb.fit_transform(df[tag + suffix])\n",
    "    \n",
    "    # Create proper column names for individual elements\n",
    "    feature_names = [f\"{tag}_{class_}\" for class_ in mlb.classes_]\n",
    "    \n",
    "    return pd.DataFrame(encoded, columns=feature_names, index=df.index)\n",
    "\n",
    "def encode_all_list_columns(df, tags, suffix='_union'):\n",
    "    \"\"\"Encode multiple list columns and concatenate into single DataFrame\"\"\"\n",
    "    encoded_dfs = []\n",
    "    \n",
    "    for tag in tags:\n",
    "        encoded = encode_list_column(df, tag, suffix=suffix)\n",
    "        encoded_dfs.append(encoded)\n",
    "    \n",
    "    # Combine all encoded features\n",
    "    features_df = pd.concat(encoded_dfs, axis=1)\n",
    "    print(f\"\\nTotal features created: {features_df.shape[1]}\")\n",
    "    return features_df\n",
    "\n",
    "# Usage\n",
    "df_encoded = encode_all_list_columns(df_clean, tags)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit kNN model\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
    "knn.fit(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query_tags(user_tags, df=df_encoded):\n",
    "    \"\"\"Encode user query tags into the same format as the training data\"\"\"\n",
    "    df_query = pd.DataFrame(0, index=[0], columns=df.columns)\n",
    "    for k, l in user_tags.items():\n",
    "        for v in l:\n",
    "            df_query.loc[0, f\"{k}_{v}\"] = 1\n",
    "    return df_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tags = {'colours': ['black'], 'types': ['pants'], 'styles': ['casual'], 'seasons': ['summer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for query tags\n",
    "query_encoded = encode_query_tags(user_tags)\n",
    "query_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(query_encoded)\n",
    "distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "axes = axes.flatten()  # Convert 2D array to 1D for easier indexing\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < distances.shape[1]:  # Ensure we don't exceed available images\n",
    "        img = mpimg.imread(str(image_list[indices[0,i]]))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{image_list[i].name}\", fontsize=8)  # Optional: show filename\n",
    "        ax.axis('off')  # Remove axes\n",
    "    else:\n",
    "        ax.axis('off')  # Hide empty subplots\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.kneighbors(query_encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-mirror-on-cloud-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
